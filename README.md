# RAG Assistant - Avinashh Resume Chatbot

A Retrieval-Augmented Generation (RAG) system that allows you to ask questions about Avinashh's resume using natural language.

## ğŸ—ï¸ Project Structure

```
Avinashh-RAG/
â”œâ”€â”€ backend/              # Backend Python code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py            # Main application entry point
â”‚   â”œâ”€â”€ api.py            # Flask server and API endpoints
â”‚   â”œâ”€â”€ index_documents.py # Document indexing script
â”‚   â”œâ”€â”€ rag_utils.py      # RAG retrieval and prompt building
â”‚   â””â”€â”€ serve_models.py   # LLM model integration (Groq API)
â”œâ”€â”€ frontend/             # Frontend files
â”‚   â”œâ”€â”€ templates/        # HTML templates (Flask)
â”‚   â”‚   â””â”€â”€ index.html   # Main chat interface
â”‚   â””â”€â”€ static/          # Static files (CSS, JS)
â”‚       â”œâ”€â”€ css/
â”‚       â”‚   â””â”€â”€ style.css # WhatsApp-style styling
â”‚       â””â”€â”€ js/
â”‚           â””â”€â”€ chat.js   # Frontend JavaScript
â”œâ”€â”€ data/                 # Documents to index (PDFs, TXT, MD)
â”‚   â””â”€â”€ resume.pdf
â”œâ”€â”€ embeddings/           # Generated vector index files
â”‚   â”œâ”€â”€ resume.index     # FAISS vector index
â”‚   â””â”€â”€ resume_meta.json # Metadata for indexed chunks
â”œâ”€â”€ venv/                # Python virtual environment
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ start_server.bat     # Windows batch script to start server
â”œâ”€â”€ start_server.ps1     # PowerShell script to start server
â””â”€â”€ README.md           # This file
```

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.8+**
2. **Groq API Key** - Get from https://console.groq.com

### Installation

1. **Clone or navigate to the project directory**

2. **Create and activate virtual environment** (if not already done)
   ```powershell
   python -m venv venv
   .\venv\Scripts\Activate.ps1  # Windows PowerShell
   ```

3. **Install dependencies**
   ```powershell
   pip install -r requirements.txt
   ```

4. **Index documents**
   ```powershell
   python backend/index_documents.py
   ```
   This reads all PDFs/text files from `data/` and creates the vector index.

5. **Start the server**
   ```powershell
   python backend/app.py
   ```
   Or use the convenience scripts:
   ```powershell
   .\start_server.ps1    # PowerShell
   .\start_server.bat    # Command Prompt
   ```
   Server runs on `http://localhost:7860`

6. **Open in browser**
   Navigate to: `http://localhost:7860`

## ğŸ“ Usage

1. **Start the backend server**
   ```powershell
   python src/backend.py
   ```

2. **Access the web interface**
   - Open `http://localhost:7860` in your browser
   - The modern UI will load automatically

3. **Ask questions**
   - Type questions about the resume content
   - Examples:
     - "What skills does Avinashh have?"
     - "Tell me about Avinashh's experience"
     - "What is Avinashh's education background?"

## ğŸ”§ Configuration

### Changing the LLM Model

Edit `backend/serve_models.py` to change the Groq model:
```python
GROQ_MODEL = "llama-3.1-70b-versatile"  # Change to your preferred Groq model
```

### Adjusting Retrieval

Edit `src/rag_utils.py` to change:
- Number of retrieved contexts: `top_k=3` in `retrieve()` function
- Embedding model: `"all-MiniLM-L6-v2"` in `SentenceTransformer()`

### Adjusting Chunking

Edit `src/index_documents.py`:
- Chunk size: `chunk_size=300` in `chunk_text()`
- Overlap: `overlap=50` in `chunk_text()`

## ğŸ”Œ API Endpoints

- `GET /` - Main web interface
- `POST /chat` - Send a question and get an answer
  ```json
  {
    "question": "What skills does Avinashh have?"
  }
  ```
  Response:
  ```json
  {
    "answer": "Based on the resume, Avinashh has..."
  }
  ```
- `GET /health` - Health check endpoint

## ğŸ› ï¸ Development

### Project Structure Explained

- **Backend (Flask)**: Serves the web interface and handles API requests
- **RAG Pipeline**: 
  1. User asks a question
  2. Question is embedded using Sentence Transformers
  3. Similar chunks are retrieved from FAISS index
  4. Context is built into a prompt
  5. LLM (Groq API) generates an answer
- **Frontend**: Modern, responsive chat interface with real-time updates

### Adding New Documents

1. Place PDF, TXT, or MD files in the `data/` folder
2. Re-run indexing: `python src/index_documents.py`
3. The new content will be searchable immediately

## ğŸ“¦ Dependencies

- **flask**: Web framework
- **flask-cors**: CORS support
- **sentence-transformers**: Text embeddings
- **faiss-cpu**: Vector similarity search
- **pdfplumber**: PDF text extraction
- **requests**: HTTP library for Groq API calls

## ğŸ› Troubleshooting

### Server won't start
- Check if dependencies are installed: `pip install -r requirements.txt`
- Verify API key is set (environment variable GROQ_API_KEY or in `backend/serve_models.py`)
- Check if embeddings exist in `embeddings/` folder

### No answers generated
- Verify Groq API connection: Check `/health` endpoint
- Check server logs for API errors
- Ensure documents are indexed
- Verify you have API credits/quota with Groq (https://console.groq.com)

### Import errors
- Make sure virtual environment is activated
- Reinstall dependencies: `pip install -r requirements.txt`

## ğŸ“„ License

This project is for personal/educational use.

